{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8407467-345b-4bcb-90fe-009d373a8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# old_combinations是一个二维数组，根据上一步生成的项集组合，结合频繁项集性质，生成新的项集组合\n",
    "def generate_new_combinations(old_combinations):\n",
    "    # 二维数组扁平化后去重，得到所有项的类型\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]\n",
    "        # mask是个True和False组成的数组\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        for item in valid_items:\n",
    "            yield from old_tuple\n",
    "            yield item\n",
    "\n",
    "\n",
    "# 速度更快，内存占用低，逐行处理，有效利用缓存\n",
    "def generate_new_combinations_low_memory(old_combinations, X, min_support):\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    rows_count = X.shape[0]\n",
    "    threshold = min_support * rows_count\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        mask_rows = X[:, old_tuple].all(axis=1)\n",
    "        supports = X[mask_rows][:, valid_items].sum(axis=0)\n",
    "        valid_indices = (supports >= threshold).nonzero()[0]\n",
    "        for index in valid_indices:\n",
    "            yield supports[index]\n",
    "            yield from old_tuple\n",
    "            yield valid_items[index]\n",
    "\n",
    "\n",
    "# verbose 0：不打印任何输出信息。\n",
    "# 1：打印算法执行过程中的一些关键信息，如当前正在处理的候选项集的大小。\n",
    "def apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False):\n",
    "    # 对x沿着纵轴求和，1的个数除以总的行数\n",
    "    def _support(_x, _n_rows):\n",
    "        out = np.sum(_x, axis=0) / _n_rows\n",
    "        #如果 out 不是一个标准的 NumPy 数组，那么使用 np.array(out).reshape(-1) 会将其转换为标准的 NumPy 数组，并且重新塑形为一维数组。\n",
    "        return np.array(out).reshape(-1)\n",
    "\n",
    "    if min_support <= 0.0:\n",
    "        raise ValueError(\"最小支持度应当为正数 %s.\" % min_support)\n",
    "\n",
    "    # [行,列]\n",
    "    X = df.values\n",
    "    support = _support(X, X.shape[0])\n",
    "    ary_col_idx = np.arange(X.shape[1])\n",
    "    \n",
    "    # support_dict 是一个字典，键为项集的大小（这里是 1），值为支持度不低于 min_support 的频繁项集的支持度。一维数组\n",
    "    # itemset_dict 是一个字典，键为项集的大小（这里是 1），值为支持度不低于 min_support 的频繁项集的列索引,二维数组\n",
    "    # support_dict和itemset_dict是一一对应的，键为项集大小，itemset_dict值为满足最小支持度的列索引，support_dict值为满足最小支持度的列的支持度值\n",
    "    # 值的类型都是键值对，value为二维数组\n",
    "    support_dict = {1: support[support >= min_support]}\n",
    "    itemset_dict = {1: ary_col_idx[support >= min_support].reshape(-1, 1)}\n",
    "    max_itemset = 1\n",
    "    rows_count = float(X.shape[0])\n",
    "    # all_ones.是一个二维数组，（9742，1），9742行，1列的二维数组，每个元素值为一\n",
    "    all_ones = np.ones((int(rows_count), 1))\n",
    "\n",
    "    while max_itemset and max_itemset < (max_len or float(\"inf\")):\n",
    "        next_max_itemset = max_itemset + 1\n",
    "        \n",
    "        #低内存模式 \n",
    "        if low_memory:\n",
    "            combin = generate_new_combinations_low_memory(\n",
    "                itemset_dict[max_itemset], X, min_support\n",
    "            )\n",
    "            # slightly faster than creating an array from a list of tuples\n",
    "            combin = np.fromiter(combin, dtype=int)\n",
    "            combin = combin.reshape(-1, next_max_itemset + 1)\n",
    "\n",
    "            if combin.size == 0:\n",
    "                break\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"\\rProcessing %d combinations | Sampling itemset size %d\"\n",
    "                    % (combin.size, next_max_itemset),\n",
    "                    end=\"\",\n",
    "                )\n",
    "\n",
    "            itemset_dict[next_max_itemset] = combin[:, 1:]\n",
    "            support_dict[next_max_itemset] = combin[:, 0].astype(float) / rows_count\n",
    "            max_itemset = next_max_itemset\n",
    "            \n",
    "        else:\n",
    "            # combin是一个二维数组，存储的是候选集（二项候选集、三项候选集等）\n",
    "            # 生成频繁一项集，然后利用频繁一项集来生成候选二项集，再根据候选二项集来生成频繁二项集，然后利用频繁二项集来生成候选三项集，\n",
    "            combin = generate_new_combinations(itemset_dict[max_itemset])\n",
    "            combin = np.fromiter(combin, dtype=int)\n",
    "            combin = combin.reshape(-1, next_max_itemset)\n",
    "            if combin.size == 0:\n",
    "                break\n",
    "            if verbose:\n",
    "                print(\"\\rProcessing %d combinations | Sampling itemset size %d\" % (combin.size, next_max_itemset),end=\"\",)\n",
    "            \n",
    "            _bools = np.all(X[:, combin], axis=2)\n",
    "            \n",
    "            support = _support(np.array(_bools), rows_count)\n",
    "            _mask = (support >= min_support).reshape(-1)\n",
    "            if any(_mask):\n",
    "                itemset_dict[next_max_itemset] = np.array(combin[_mask])\n",
    "                support_dict[next_max_itemset] = np.array(support[_mask])\n",
    "                max_itemset = next_max_itemset\n",
    "            else:\n",
    "                # Exit condition\n",
    "                break\n",
    "\n",
    "    all_res = []\n",
    "    for k in sorted(itemset_dict):\n",
    "        support = pd.Series(support_dict[k])\n",
    "        itemsets = pd.Series([frozenset(i) for i in itemset_dict[k]], dtype=\"object\")\n",
    "\n",
    "        res = pd.concat((support, itemsets), axis=1)\n",
    "        all_res.append(res)\n",
    "\n",
    "    res_df = pd.concat(all_res)\n",
    "    res_df.columns = [\"support\", \"itemsets\"]\n",
    "    if use_colnames:\n",
    "        mapping = {idx: item for idx, item in enumerate(df.columns)}\n",
    "        res_df[\"itemsets\"] = res_df[\"itemsets\"].apply(\n",
    "            lambda x: frozenset([mapping[i] for i in x])\n",
    "        )\n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print()  # adds newline if verbose counter was used\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23ad07d-ebc1-47bb-ab0a-86e2be32ed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coast:0.0040s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187641</td>\n",
       "      <td>(Action)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129645</td>\n",
       "      <td>(Adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062718</td>\n",
       "      <td>(Animation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068158</td>\n",
       "      <td>(Children)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.385547</td>\n",
       "      <td>(Comedy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.123075</td>\n",
       "      <td>(Crime)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.447649</td>\n",
       "      <td>(Drama)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.079963</td>\n",
       "      <td>(Fantasy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.100390</td>\n",
       "      <td>(Horror)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058817</td>\n",
       "      <td>(Mystery)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.163827</td>\n",
       "      <td>(Romance)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.100595</td>\n",
       "      <td>(Sci-Fi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.194416</td>\n",
       "      <td>(Thriller)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.062615</td>\n",
       "      <td>(Action, Adventure)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.054301</td>\n",
       "      <td>(Action, Drama)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.067235</td>\n",
       "      <td>(Thriller, Action)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.103983</td>\n",
       "      <td>(Comedy, Drama)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.090741</td>\n",
       "      <td>(Romance, Comedy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.065387</td>\n",
       "      <td>(Crime, Drama)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.058407</td>\n",
       "      <td>(Crime, Thriller)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.095874</td>\n",
       "      <td>(Romance, Drama)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.085403</td>\n",
       "      <td>(Thriller, Drama)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     support             itemsets\n",
       "0   0.187641             (Action)\n",
       "1   0.129645          (Adventure)\n",
       "2   0.062718          (Animation)\n",
       "3   0.068158           (Children)\n",
       "4   0.385547             (Comedy)\n",
       "5   0.123075              (Crime)\n",
       "6   0.447649              (Drama)\n",
       "7   0.079963            (Fantasy)\n",
       "8   0.100390             (Horror)\n",
       "9   0.058817            (Mystery)\n",
       "10  0.163827            (Romance)\n",
       "11  0.100595             (Sci-Fi)\n",
       "12  0.194416           (Thriller)\n",
       "13  0.062615  (Action, Adventure)\n",
       "14  0.054301      (Action, Drama)\n",
       "15  0.067235   (Thriller, Action)\n",
       "16  0.103983      (Comedy, Drama)\n",
       "17  0.090741    (Romance, Comedy)\n",
       "18  0.065387       (Crime, Drama)\n",
       "19  0.058407    (Crime, Thriller)\n",
       "20  0.095874     (Romance, Drama)\n",
       "21  0.085403    (Thriller, Drama)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "movies = pd.read_csv(r'C:\\Users\\86131\\Desktop\\毕设\\data\\movies.csv')\n",
    "movies = movies.drop('genres', axis=1).join(movies.genres.str.get_dummies())\n",
    "movies.set_index(['movieId','title'], inplace=True)\n",
    "t=time.time()\n",
    "ret = apriori(movies,min_support=0.05,use_colnames=True,low_memory=True)\n",
    "print(f'coast:{time.time() - t:.4f}s')\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235eddd4-e8dd-444b-9258-b61808ce2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def association_rules(df, metric=\"confidence\", min_threshold=0.8, support_only=False):\n",
    "    if not df.shape[0]:\n",
    "        raise ValueError(\n",
    "            \"The input DataFrame `df` containing \" \"the frequent itemsets is empty.\"\n",
    "        )\n",
    "\n",
    "    # check for mandatory columns\n",
    "    if not all(col in df.columns for col in [\"support\", \"itemsets\"]):\n",
    "        raise ValueError(\n",
    "            \"Dataframe needs to contain the\\\n",
    "                         columns 'support' and 'itemsets'\"\n",
    "        )\n",
    "\n",
    "    def conviction_helper(sAC, sA, sC):\n",
    "        confidence = sAC / sA\n",
    "        conviction = np.empty(confidence.shape, dtype=float)\n",
    "        if not len(conviction.shape):\n",
    "            conviction = conviction[np.newaxis]\n",
    "            confidence = confidence[np.newaxis]\n",
    "            sAC = sAC[np.newaxis]\n",
    "            sA = sA[np.newaxis]\n",
    "            sC = sC[np.newaxis]\n",
    "        conviction[:] = np.inf\n",
    "        conviction[confidence < 1.0] = (1.0 - sC[confidence < 1.0]) / (\n",
    "            1.0 - confidence[confidence < 1.0]\n",
    "        )\n",
    "\n",
    "        return conviction\n",
    "\n",
    "    def zhangs_metric_helper(sAC, sA, sC):\n",
    "        denominator = np.maximum(sAC * (1 - sA), sA * (sC - sAC))\n",
    "        numerator = metric_dict[\"leverage\"](sAC, sA, sC)\n",
    "\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            # ignoring the divide by 0 warning since it is addressed in the below np.where\n",
    "            zhangs_metric = np.where(denominator == 0, 0, numerator / denominator)\n",
    "\n",
    "        return zhangs_metric\n",
    "\n",
    "    # metrics for association rules\n",
    "    metric_dict = {\n",
    "        \"antecedent support\": lambda _, sA, __: sA,\n",
    "        \"consequent support\": lambda _, __, sC: sC,\n",
    "        \"support\": lambda sAC, _, __: sAC,\n",
    "        \"confidence\": lambda sAC, sA, _: sAC / sA,\n",
    "        \"lift\": lambda sAC, sA, sC: metric_dict[\"confidence\"](sAC, sA, sC) / sC,\n",
    "        \"leverage\": lambda sAC, sA, sC: metric_dict[\"support\"](sAC, sA, sC) - sA * sC,\n",
    "        \"conviction\": lambda sAC, sA, sC: conviction_helper(sAC, sA, sC),\n",
    "        \"zhangs_metric\": lambda sAC, sA, sC: zhangs_metric_helper(sAC, sA, sC),\n",
    "    }\n",
    "\n",
    "    columns_ordered = [\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\",\n",
    "        \"support\",\n",
    "        \"confidence\",\n",
    "        \"lift\",\n",
    "        \"leverage\",\n",
    "        \"conviction\",\n",
    "        \"zhangs_metric\",\n",
    "    ]\n",
    "\n",
    "    # check for metric compliance\n",
    "    if support_only:\n",
    "        metric = \"support\"\n",
    "    else:\n",
    "        if metric not in metric_dict.keys():\n",
    "            raise ValueError(\n",
    "                \"Metric must be 'confidence' or 'lift', got '{}'\".format(metric)\n",
    "            )\n",
    "\n",
    "    # get dict of {frequent itemset} -> support\n",
    "    keys = df[\"itemsets\"].values\n",
    "    values = df[\"support\"].values\n",
    "    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n",
    "    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n",
    "\n",
    "    # prepare buckets to collect frequent rules\n",
    "    rule_antecedents = []\n",
    "    rule_consequents = []\n",
    "    rule_supports = []\n",
    "\n",
    "    # iterate over all frequent itemsets\n",
    "    for k in frequent_items_dict.keys():\n",
    "        sAC = frequent_items_dict[k]\n",
    "        # to find all possible combinations\n",
    "        for idx in range(len(k) - 1, 0, -1):\n",
    "            # of antecedent and consequent\n",
    "            for c in combinations(k, r=idx):\n",
    "                antecedent = frozenset(c)\n",
    "                consequent = k.difference(antecedent)\n",
    "\n",
    "                if support_only:\n",
    "                    # support doesn't need these,\n",
    "                    # hence, placeholders should suffice\n",
    "                    sA = None\n",
    "                    sC = None\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        sA = frequent_items_dict[antecedent]\n",
    "                        sC = frequent_items_dict[consequent]\n",
    "                    except KeyError as e:\n",
    "                        s = (\n",
    "                            str(e) + \"You are likely getting this error\"\n",
    "                            \" because the DataFrame is missing \"\n",
    "                            \" antecedent and/or consequent \"\n",
    "                            \" information.\"\n",
    "                            \" You can try using the \"\n",
    "                            \" `support_only=True` option\"\n",
    "                        )\n",
    "                        raise KeyError(s)\n",
    "                    # check for the threshold\n",
    "\n",
    "                score = metric_dict[metric](sAC, sA, sC)\n",
    "                if score >= min_threshold:\n",
    "                    rule_antecedents.append(antecedent)\n",
    "                    rule_consequents.append(consequent)\n",
    "                    rule_supports.append([sAC, sA, sC])\n",
    "\n",
    "    # check if frequent rule was generated\n",
    "    if not rule_supports:\n",
    "        return pd.DataFrame(columns=[\"antecedents\", \"consequents\"] + columns_ordered)\n",
    "\n",
    "    else:\n",
    "        # generate metrics\n",
    "        rule_supports = np.array(rule_supports).T.astype(float)\n",
    "        df_res = pd.DataFrame(\n",
    "            data=list(zip(rule_antecedents, rule_consequents)),\n",
    "            columns=[\"antecedents\", \"consequents\"],\n",
    "        )\n",
    "\n",
    "        if support_only:\n",
    "            sAC = rule_supports[0]\n",
    "            for m in columns_ordered:\n",
    "                df_res[m] = np.nan\n",
    "            df_res[\"support\"] = sAC\n",
    "\n",
    "        else:\n",
    "            sAC = rule_supports[0]\n",
    "            sA = rule_supports[1]\n",
    "            sC = rule_supports[2]\n",
    "            for m in columns_ordered:\n",
    "                df_res[m] = metric_dict[m](sAC, sA, sC)\n",
    "\n",
    "        return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7e1785-7db9-4ddc-aba5-0eed6a26aba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Action)</td>\n",
       "      <td>(Adventure)</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.129645</td>\n",
       "      <td>0.062615</td>\n",
       "      <td>0.333698</td>\n",
       "      <td>2.573940</td>\n",
       "      <td>0.038289</td>\n",
       "      <td>1.306247</td>\n",
       "      <td>0.752735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Adventure)</td>\n",
       "      <td>(Action)</td>\n",
       "      <td>0.129645</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.062615</td>\n",
       "      <td>0.482977</td>\n",
       "      <td>2.573940</td>\n",
       "      <td>0.038289</td>\n",
       "      <td>1.571224</td>\n",
       "      <td>0.702576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>(Action)</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>0.345829</td>\n",
       "      <td>1.843034</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>1.241814</td>\n",
       "      <td>0.567807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Action)</td>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>0.358315</td>\n",
       "      <td>1.843034</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>1.255420</td>\n",
       "      <td>0.563072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Comedy)</td>\n",
       "      <td>(Romance)</td>\n",
       "      <td>0.385547</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.090741</td>\n",
       "      <td>0.235357</td>\n",
       "      <td>1.436620</td>\n",
       "      <td>0.027578</td>\n",
       "      <td>1.093547</td>\n",
       "      <td>0.494622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Romance)</td>\n",
       "      <td>(Comedy)</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.385547</td>\n",
       "      <td>0.090741</td>\n",
       "      <td>0.553885</td>\n",
       "      <td>1.436620</td>\n",
       "      <td>0.027578</td>\n",
       "      <td>1.377341</td>\n",
       "      <td>0.363467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Crime)</td>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>0.123075</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.058407</td>\n",
       "      <td>0.474562</td>\n",
       "      <td>2.440963</td>\n",
       "      <td>0.034479</td>\n",
       "      <td>1.533167</td>\n",
       "      <td>0.673177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>(Crime)</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.123075</td>\n",
       "      <td>0.058407</td>\n",
       "      <td>0.300422</td>\n",
       "      <td>2.440963</td>\n",
       "      <td>0.034479</td>\n",
       "      <td>1.253506</td>\n",
       "      <td>0.732792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Romance)</td>\n",
       "      <td>(Drama)</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.447649</td>\n",
       "      <td>0.095874</td>\n",
       "      <td>0.585213</td>\n",
       "      <td>1.307302</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>1.331649</td>\n",
       "      <td>0.281121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Drama)</td>\n",
       "      <td>(Romance)</td>\n",
       "      <td>0.447649</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.095874</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>1.307302</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>1.064065</td>\n",
       "      <td>0.425574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antecedents  consequents  antecedent support  consequent support   support  \\\n",
       "0     (Action)  (Adventure)            0.187641            0.129645  0.062615   \n",
       "1  (Adventure)     (Action)            0.129645            0.187641  0.062615   \n",
       "2   (Thriller)     (Action)            0.194416            0.187641  0.067235   \n",
       "3     (Action)   (Thriller)            0.187641            0.194416  0.067235   \n",
       "4     (Comedy)    (Romance)            0.385547            0.163827  0.090741   \n",
       "5    (Romance)     (Comedy)            0.163827            0.385547  0.090741   \n",
       "6      (Crime)   (Thriller)            0.123075            0.194416  0.058407   \n",
       "7   (Thriller)      (Crime)            0.194416            0.123075  0.058407   \n",
       "8    (Romance)      (Drama)            0.163827            0.447649  0.095874   \n",
       "9      (Drama)    (Romance)            0.447649            0.163827  0.095874   \n",
       "\n",
       "   confidence      lift  leverage  conviction  zhangs_metric  \n",
       "0    0.333698  2.573940  0.038289    1.306247       0.752735  \n",
       "1    0.482977  2.573940  0.038289    1.571224       0.702576  \n",
       "2    0.345829  1.843034  0.030754    1.241814       0.567807  \n",
       "3    0.358315  1.843034  0.030754    1.255420       0.563072  \n",
       "4    0.235357  1.436620  0.027578    1.093547       0.494622  \n",
       "5    0.553885  1.436620  0.027578    1.377341       0.363467  \n",
       "6    0.474562  2.440963  0.034479    1.533167       0.673177  \n",
       "7    0.300422  2.440963  0.034479    1.253506       0.732792  \n",
       "8    0.585213  1.307302  0.022537    1.331649       0.281121  \n",
       "9    0.214171  1.307302  0.022537    1.064065       0.425574  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = association_rules(ret, metric='lift', min_threshold=1.25)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f01bb4-cd37-4e93-8fcd-293e25a4d108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Adventure)</td>\n",
       "      <td>(Action)</td>\n",
       "      <td>0.129645</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.062615</td>\n",
       "      <td>0.482977</td>\n",
       "      <td>2.573940</td>\n",
       "      <td>0.038289</td>\n",
       "      <td>1.571224</td>\n",
       "      <td>0.702576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Action)</td>\n",
       "      <td>(Adventure)</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.129645</td>\n",
       "      <td>0.062615</td>\n",
       "      <td>0.333698</td>\n",
       "      <td>2.573940</td>\n",
       "      <td>0.038289</td>\n",
       "      <td>1.306247</td>\n",
       "      <td>0.752735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Crime)</td>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>0.123075</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.058407</td>\n",
       "      <td>0.474562</td>\n",
       "      <td>2.440963</td>\n",
       "      <td>0.034479</td>\n",
       "      <td>1.533167</td>\n",
       "      <td>0.673177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>(Crime)</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.123075</td>\n",
       "      <td>0.058407</td>\n",
       "      <td>0.300422</td>\n",
       "      <td>2.440963</td>\n",
       "      <td>0.034479</td>\n",
       "      <td>1.253506</td>\n",
       "      <td>0.732792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>(Action)</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>0.345829</td>\n",
       "      <td>1.843034</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>1.241814</td>\n",
       "      <td>0.567807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Action)</td>\n",
       "      <td>(Thriller)</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.194416</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>0.358315</td>\n",
       "      <td>1.843034</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>1.255420</td>\n",
       "      <td>0.563072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Comedy)</td>\n",
       "      <td>(Romance)</td>\n",
       "      <td>0.385547</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.090741</td>\n",
       "      <td>0.235357</td>\n",
       "      <td>1.436620</td>\n",
       "      <td>0.027578</td>\n",
       "      <td>1.093547</td>\n",
       "      <td>0.494622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Romance)</td>\n",
       "      <td>(Comedy)</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.385547</td>\n",
       "      <td>0.090741</td>\n",
       "      <td>0.553885</td>\n",
       "      <td>1.436620</td>\n",
       "      <td>0.027578</td>\n",
       "      <td>1.377341</td>\n",
       "      <td>0.363467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Romance)</td>\n",
       "      <td>(Drama)</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.447649</td>\n",
       "      <td>0.095874</td>\n",
       "      <td>0.585213</td>\n",
       "      <td>1.307302</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>1.331649</td>\n",
       "      <td>0.281121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Drama)</td>\n",
       "      <td>(Romance)</td>\n",
       "      <td>0.447649</td>\n",
       "      <td>0.163827</td>\n",
       "      <td>0.095874</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>1.307302</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>1.064065</td>\n",
       "      <td>0.425574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antecedents  consequents  antecedent support  consequent support   support  \\\n",
       "1  (Adventure)     (Action)            0.129645            0.187641  0.062615   \n",
       "0     (Action)  (Adventure)            0.187641            0.129645  0.062615   \n",
       "6      (Crime)   (Thriller)            0.123075            0.194416  0.058407   \n",
       "7   (Thriller)      (Crime)            0.194416            0.123075  0.058407   \n",
       "2   (Thriller)     (Action)            0.194416            0.187641  0.067235   \n",
       "3     (Action)   (Thriller)            0.187641            0.194416  0.067235   \n",
       "4     (Comedy)    (Romance)            0.385547            0.163827  0.090741   \n",
       "5    (Romance)     (Comedy)            0.163827            0.385547  0.090741   \n",
       "8    (Romance)      (Drama)            0.163827            0.447649  0.095874   \n",
       "9      (Drama)    (Romance)            0.447649            0.163827  0.095874   \n",
       "\n",
       "   confidence      lift  leverage  conviction  zhangs_metric  \n",
       "1    0.482977  2.573940  0.038289    1.571224       0.702576  \n",
       "0    0.333698  2.573940  0.038289    1.306247       0.752735  \n",
       "6    0.474562  2.440963  0.034479    1.533167       0.673177  \n",
       "7    0.300422  2.440963  0.034479    1.253506       0.732792  \n",
       "2    0.345829  1.843034  0.030754    1.241814       0.567807  \n",
       "3    0.358315  1.843034  0.030754    1.255420       0.563072  \n",
       "4    0.235357  1.436620  0.027578    1.093547       0.494622  \n",
       "5    0.553885  1.436620  0.027578    1.377341       0.363467  \n",
       "8    0.585213  1.307302  0.022537    1.331649       0.281121  \n",
       "9    0.214171  1.307302  0.022537    1.064065       0.425574  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对lift降序排序，查看lift较大的是哪些规则\n",
    "rules_sort = rules.sort_values(by=['lift'], ascending=False)\n",
    "rules_sort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
